{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1RCQy-EsbQIxQP1iG5DWo56r14UDUgc_S","timestamp":1702645708132}],"collapsed_sections":["saQatSyGWs1C","l5nKq349kjh3","ntJWUrrzPK3K","oKgWhM0ekKVH","B8NDUh3C4FSe","tIELDPwPxVE6","A3TxFnfXxxjC","wObYbu0OyQef","tYyg1BsVyor-","cHGJzNsQy8IA","E24dr21bzK1e","QbiGW_WPzaR_","jlmbtQBlzm3w","ltMhTTKcjota","qWCEuukijsXB"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Duplication issue\n","The following pollution functions are used to inject non-exact duplicates into the dataset, for a total of 10 different experiments."],"metadata":{"id":"1Vid0xbKY1qq"}},{"cell_type":"markdown","source":["### Imports"],"metadata":{"id":"saQatSyGWs1C"}},{"cell_type":"code","source":["!pip install recordlinkage"],"metadata":{"id":"FXk_ZHE-lhkp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random as rnd\n","import pandas as pd\n","import numpy as np\n","import numpy.linalg as la\n","import string\n","from datetime import datetime, timedelta\n","from scipy.stats import pearsonr\n","import recordlinkage\n","# -- IMPORT SCRIPTS FROM DATADIQ REPO (@camillasancricca)\n","import A_data_collection as data_collection\n","import D_data_analysis as data_analysis\n","import E_plot_results as plot_results"],"metadata":{"id":"pN2NJ-FK5iLU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SEED = 2023\n","NUM_EXPERIMENTS = 10\n","\n","np.random.seed(SEED)\n","rnd.seed(SEED)"],"metadata":{"id":"wj0RC4zelZux"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["INIT_FEATURES = 5\n","NUM_INFORMATIVE = 5\n","NUM_REDUNDANT = INIT_FEATURES - NUM_INFORMATIVE\n","assert NUM_REDUNDANT >= 0"],"metadata":{"id":"ON8qFNdJlc4J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dataset for usage example\n","example_dataset, example_labels = data_collection.make_dataset_for_classification(n_samples=5, n_features=INIT_FEATURES, n_informative=NUM_INFORMATIVE, n_redundant=NUM_REDUNDANT, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, seed=2023)\n","# dataset for duplication experiments\n","X_dup, y_dup = data_collection.make_dataset_for_classification(n_samples=1000, n_features=INIT_FEATURES, n_informative=NUM_INFORMATIVE, n_redundant=NUM_REDUNDANT, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, seed=2023)"],"metadata":{"id":"bz_jZLucldbi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Pollution Functions"],"metadata":{"id":"S17AdkP9kDrd"}},{"cell_type":"markdown","source":["#### Rounding-off pollution function\n","This pollution function randomly picks a percentage of entries from the dataset, duplicates them, and shaves off a random amount of digits, to a random digit, for a random amount of features."],"metadata":{"id":"mjvF5a1MZUtR"}},{"cell_type":"code","source":["def pollute_round_off(dataset, labels, percentage):\n","\n","  '''\n","  Create duplicates from the dataset of size (percentage * dataset.shape[0]) and\n","   apply rounding off to a random number of features with random digits.\n","\n","  Parameters:\n","    dataset (numpy.ndarray): The dataset to be duplicated and polluted.\n","    labels (numpy.ndarray): The labels of the dataset.\n","    percentage (float): The fraction of the dataset size to duplicate.\n","\n","  Returns:\n","    numpy.ndarray: The polluted dataset with duplicates.\n","\n","  Raises:\n","    ValueError: If the dataset is empty or if the percentage is not between 0 and 1.\n","  '''\n","\n","  if not (0 < percentage < 1):\n","    raise ValueError(\"Percentage must be between 0 and 1.\")\n","\n","  num_entries, num_features = dataset.shape\n","\n","  if num_entries <= 0:\n","    raise ValueError(\"Dataset must be non-empty.\")\n","\n","  num_duplicates = int(num_entries * percentage)\n","\n","  if num_duplicates == 0:\n","    num_duplicates = 1\n","\n","  # Select random entries to duplicate and create a deep copy\n","  duplicate_indices = rnd.sample(range(num_entries), num_duplicates)\n","  duplicate_data = dataset[duplicate_indices].copy()\n","  duplicate_labels = labels[duplicate_indices].copy()\n","\n","  for entry in duplicate_data:\n","    num_round_features = rnd.randint(1, num_features)\n","\n","    round_features = rnd.sample(range(num_features), num_round_features)\n","\n","    for feature in round_features:\n","      # Calculate the digits of a data point\n","      entry_digits = len(str(entry[feature]).split('.')[1]) - 1 # Remove digit before comma\n","\n","      # Randomly select how many digits to round off\n","      digits = rnd.randint(1, entry_digits - 1) # Remove 1 to avoid chance of exact duplicates (Extremes are included in randint)\n","      entry[feature] = np.round(entry[feature], digits) # Round entry to generated digit\n","\n","  return (np.append(dataset, duplicate_data, axis=0), np.append(labels, duplicate_labels, axis=0))"],"metadata":{"id":"xlMOewLxZOLh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Usage example**"],"metadata":{"id":"28fl9FonZ1Sg"}},{"cell_type":"code","source":["percentage = .5\n","\n","polluted_dataset, polluted_labels = pollute_round_off(example_dataset, example_labels, percentage)"],"metadata":{"id":"K0dYAm3PZ_w4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example_dataset, example_labels"],"metadata":{"id":"EqFWxPFSaDAb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704033289082,"user_tz":-60,"elapsed":11,"user":{"displayName":"Federico Toschi","userId":"03268067091644128077"}},"outputId":"08e55703-a103-475b-fad2-ab592c94a7e8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[-0.94920925,  0.33568484, -1.41548407, -1.78576295, -0.48758674],\n","        [-3.35471844, -1.4895879 ,  0.47891255, -2.63812044,  0.25589926],\n","        [ 2.8429093 ,  0.83633122, -1.89775432,  1.68994257, -0.59224401],\n","        [ 0.80460305,  2.24711467, -3.28064942, -1.0454157 , -1.63415213],\n","        [-0.89816726,  1.39373349, -0.79674393,  0.17159317,  0.02322873]]),\n"," array([1, 0, 1, 0, 0]))"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["polluted_dataset, polluted_labels"],"metadata":{"id":"uxtMhFK0aFfz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704033289082,"user_tz":-60,"elapsed":10,"user":{"displayName":"Federico Toschi","userId":"03268067091644128077"}},"outputId":"54729d03-d178-4613-d6c5-24dc2d024978"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[-0.94920925,  0.33568484, -1.41548407, -1.78576295, -0.48758674],\n","        [-3.35471844, -1.4895879 ,  0.47891255, -2.63812044,  0.25589926],\n","        [ 2.8429093 ,  0.83633122, -1.89775432,  1.68994257, -0.59224401],\n","        [ 0.80460305,  2.24711467, -3.28064942, -1.0454157 , -1.63415213],\n","        [-0.89816726,  1.39373349, -0.79674393,  0.17159317,  0.02322873],\n","        [-0.94920925,  0.33568484, -1.41548407, -1.78576295, -0.49      ],\n","        [-0.8982    ,  1.4       , -0.79674393,  0.1716    ,  0.02322873]]),\n"," array([1, 0, 1, 0, 0, 1, 0]))"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["#### Gaussian Noise pollution function\n","This pollution function randomly picks a percentage of entries from the dataset, duplicates them, and applies a Gaussian noise to them."],"metadata":{"id":"R65F4dUZaPci"}},{"cell_type":"code","source":["def pollute_gaussian_noise(dataset, labels, percentage, mean=0):\n","\n","  '''\n","  Create duplicates from the dataset of size (percentage * dataset.shape[0]) and\n","   add Gaussian noise of mean MEAN and variance proportional to the dataset's\n","   variance, to each feature of each duplicate.\n","\n","  Parameters:\n","    dataset (numpy.ndarray): The dataset to be duplicated and polluted.\n","    labels (numpy.ndarray): The labels of the dataset.\n","    percentage (float): The fraction of the dataset size to duplicate.\n","    mean (float): The mean of the Gaussian noise distribution.\n","\n","  Returns:\n","    numpy.ndarray: The polluted dataset with duplicates.\n","\n","  Raises:\n","    ValueError: If the dataset is empty or if the percentage is not between 0 and 1.\n","  '''\n","\n","  if not (0 < percentage < 1):\n","    raise ValueError(\"Percentage must be between 0 and 1.\")\n","\n","  num_entries, num_features = dataset.shape\n","\n","  if num_entries <= 0:\n","    raise ValueError(\"Dataset must be non-empty.\")\n","\n","  num_duplicates = int(num_entries * percentage)\n","\n","  if num_duplicates == 0:\n","    num_duplicates = 1\n","\n","  # Calculate the standard deviation of the dataset\n","  std_dev_data = np.std(dataset)\n","\n","  # Determine a scaling factor for the noise\n","  scaling_factor = 0.1  # 10% of the data's standard deviation\n","\n","  # Calculate the standard deviation for the noise\n","  std_dev_noise = scaling_factor * std_dev_data\n","\n","  # Select random entries to duplicate and create a deep copy\n","  duplicate_indices = np.random.choice(num_entries, size=num_duplicates)\n","  duplicate_data = dataset[duplicate_indices].copy()\n","  duplicate_labels = labels[duplicate_indices].copy()\n","\n","  for entry in duplicate_data:\n","    noise = np.random.normal(mean, std_dev_noise, num_features)\n","    entry += noise\n","\n","  return (np.append(dataset, duplicate_data, axis=0), np.append(labels, duplicate_labels, axis=0))"],"metadata":{"id":"lw6aHZStaVDs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Usage example**"],"metadata":{"id":"1ZrLa3fYbaPr"}},{"cell_type":"code","source":["percentage = .5\n","\n","polluted_dataset, polluted_labels = pollute_gaussian_noise(example_dataset, example_labels, percentage)"],"metadata":{"id":"Bf1r6GMKbblX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example_dataset, example_labels"],"metadata":{"id":"Es9URlaqbpJ6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704033289083,"user_tz":-60,"elapsed":10,"user":{"displayName":"Federico Toschi","userId":"03268067091644128077"}},"outputId":"67ee0ddd-e452-4af4-abd0-16cde02a5c2a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[-0.94920925,  0.33568484, -1.41548407, -1.78576295, -0.48758674],\n","        [-3.35471844, -1.4895879 ,  0.47891255, -2.63812044,  0.25589926],\n","        [ 2.8429093 ,  0.83633122, -1.89775432,  1.68994257, -0.59224401],\n","        [ 0.80460305,  2.24711467, -3.28064942, -1.0454157 , -1.63415213],\n","        [-0.89816726,  1.39373349, -0.79674393,  0.17159317,  0.02322873]]),\n"," array([1, 0, 1, 0, 0]))"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["polluted_dataset, polluted_labels"],"metadata":{"id":"aRpSAZQrbrLm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704033289083,"user_tz":-60,"elapsed":9,"user":{"displayName":"Federico Toschi","userId":"03268067091644128077"}},"outputId":"00c68ce3-f0cf-4d01-8b1d-e14068d2d9c6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[-0.94920925,  0.33568484, -1.41548407, -1.78576295, -0.48758674],\n","        [-3.35471844, -1.4895879 ,  0.47891255, -2.63812044,  0.25589926],\n","        [ 2.8429093 ,  0.83633122, -1.89775432,  1.68994257, -0.59224401],\n","        [ 0.80460305,  2.24711467, -3.28064942, -1.0454157 , -1.63415213],\n","        [-0.89816726,  1.39373349, -0.79674393,  0.17159317,  0.02322873],\n","        [ 0.88981186,  2.150672  , -3.46835508, -0.70980104, -1.64897999],\n","        [-1.0437219 ,  1.25501095, -0.86133709,  0.09596303,  0.04895464]]),\n"," array([1, 0, 1, 0, 0, 0, 0]))"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["####Scaling pollution function\n","This pollution function randomly picks a percentage of entries from the dataset, duplicates them, and scales them to a different, randomly chosen unit of measurement."],"metadata":{"id":"S-y76wMWb6yG"}},{"cell_type":"code","source":["def pollute_scaling(dataset, labels, percentage, min_exp=-12, max_exp=12):\n","\n","  '''\n","  Create duplicates from the dataset of size (percentage * dataset.shape[0]) and\n","   scales them to a different, randomly chosen unit of measurement (i.e.,\n","   multiplies each data item for a factor that is 10^exp, where exp is a random\n","   exponent between MIN_EXP and MAX_EXP.\n","\n","  Parameters:\n","    dataset (numpy.ndarray): The dataset to be duplicated and polluted.\n","    labels (numpy.ndarray): The labels of the dataset.\n","    percentage (float): The fraction of the dataset size to duplicate.\n","    min_exp (int): The lower bound of powers of 10 for scaling pollution.\n","    max_exp (int): The upper bound of powers of 10 for scaling pollution.\n","\n","  Returns:\n","    numpy.ndarray: The polluted dataset with duplicates.\n","\n","  Raises:\n","    ValueError: If the dataset is empty or if the percentage is not between 0 and 1.\n","  '''\n","\n","  if not (0 < percentage < 1):\n","    raise ValueError(\"Percentage must be between 0 and 1.\")\n","\n","  num_entries, num_features = dataset.shape\n","\n","  if num_entries <= 0:\n","    raise ValueError(\"Dataset must be non-empty.\")\n","  num_duplicates = int(num_entries * percentage)\n","\n","  if num_duplicates == 0:\n","    num_duplicates = 1\n","\n","  # Select random entries to duplicate and create a deep copy\n","  duplicate_indices = np.random.choice(num_entries, size=num_duplicates)\n","  duplicate_data = dataset[duplicate_indices].copy()\n","  duplicate_labels = labels[duplicate_indices].copy()\n","\n","  for entry in duplicate_data:\n","      # pollute the elements of the row with the same scale\n","      exp = rnd.randint(min_exp, max_exp)\n","      entry *= (10 ** exp)\n","\n","  return (np.append(dataset, duplicate_data, axis=0), np.append(labels, duplicate_labels, axis=0))"],"metadata":{"id":"P-tTrjkheR3A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Usage example**"],"metadata":{"id":"9kuCMZEUjIh5"}},{"cell_type":"code","source":["percentage = .5\n","\n","polluted_dataset, polluted_labels = pollute_scaling(example_dataset, example_labels, percentage)"],"metadata":{"id":"AIV35yCFjK_Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example_dataset, example_labels"],"metadata":{"id":"yC4LGY0TjQG1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704033289083,"user_tz":-60,"elapsed":9,"user":{"displayName":"Federico Toschi","userId":"03268067091644128077"}},"outputId":"ffbc974d-c314-4e85-e54b-b7646c5c601c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[-0.94920925,  0.33568484, -1.41548407, -1.78576295, -0.48758674],\n","        [-3.35471844, -1.4895879 ,  0.47891255, -2.63812044,  0.25589926],\n","        [ 2.8429093 ,  0.83633122, -1.89775432,  1.68994257, -0.59224401],\n","        [ 0.80460305,  2.24711467, -3.28064942, -1.0454157 , -1.63415213],\n","        [-0.89816726,  1.39373349, -0.79674393,  0.17159317,  0.02322873]]),\n"," array([1, 0, 1, 0, 0]))"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["polluted_dataset, polluted_labels"],"metadata":{"id":"aISfZzRejTzy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704033289083,"user_tz":-60,"elapsed":8,"user":{"displayName":"Federico Toschi","userId":"03268067091644128077"}},"outputId":"e45f26a4-883f-4719-af71-dfcabb04c85f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[-9.49209246e-01,  3.35684837e-01, -1.41548407e+00,\n","         -1.78576295e+00, -4.87586737e-01],\n","        [-3.35471844e+00, -1.48958790e+00,  4.78912548e-01,\n","         -2.63812044e+00,  2.55899262e-01],\n","        [ 2.84290930e+00,  8.36331225e-01, -1.89775432e+00,\n","          1.68994257e+00, -5.92244007e-01],\n","        [ 8.04603049e-01,  2.24711467e+00, -3.28064942e+00,\n","         -1.04541570e+00, -1.63415213e+00],\n","        [-8.98167260e-01,  1.39373349e+00, -7.96743925e-01,\n","          1.71593173e-01,  2.32287279e-02],\n","        [ 8.04603049e+04,  2.24711467e+05, -3.28064942e+05,\n","         -1.04541570e+05, -1.63415213e+05],\n","        [-3.35471844e-06, -1.48958790e-06,  4.78912548e-07,\n","         -2.63812044e-06,  2.55899262e-07]]),\n"," array([1, 0, 1, 0, 0, 0, 0]))"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["####Swapping pollution function\n","This pollution function randomly picks a percentage of entries from the dataset, duplicates them, and swaps a random amount of features."],"metadata":{"id":"wh7hFB5lnA5m"}},{"cell_type":"code","source":["def pollute_swapping(dataset, labels, percentage):\n","\n","  '''\n","  Create duplicates from the dataset of size (percentage * dataset.shape[0]) and\n","   swaps a randomly chosen number of features.\n","\n","  Parameters:\n","    dataset (numpy.ndarray): The dataset to be duplicated and polluted.\n","    labels (numpy.ndarray): The labels of the dataset.\n","    percentage (float): The fraction of the dataset size to duplicate.\n","\n","  Returns:\n","    numpy.ndarray: The polluted dataset with duplicates.\n","\n","  Raises:\n","    ValueError: If the dataset is empty, if the dataset has just one feature,\n","    or if the percentage is not between 0 and 1.\n","  '''\n","\n","  if not (0 < percentage < 1):\n","    raise ValueError(\"Percentage must be between 0 and 1.\")\n","\n","  num_entries, num_features = dataset.shape\n","\n","  if num_entries <= 0:\n","    raise ValueError(\"Dataset must be non-empty.\")\n","\n","  if num_features <= 1:\n","    raise ValueError(\"Dataset must have at least two features.\")\n","\n","  num_duplicates = int(num_entries * percentage)\n","\n","  if num_duplicates == 0:\n","    num_duplicates = 1\n","\n","  # Select random entries to duplicate and create a deep copy\n","  duplicate_indices = np.random.choice(num_entries, size=num_duplicates)\n","  duplicate_data = dataset[duplicate_indices].copy()\n","  duplicate_labels = labels[duplicate_indices].copy()\n","\n","  for entry in duplicate_data:\n","\n","    # randomly choose how many times to swap\n","    num_swap_features = rnd.randint(1, num_features - 1)\n","\n","    # randomly choose the new order of features\n","    new_features_order = list(range(num_features))\n","\n","    old_entry = entry.copy()\n","    while np.array_equal(old_entry, entry):\n","      for _ in range(num_swap_features):\n","        index1 = 0\n","        index2 = 0\n","        while index1 == index2:\n","          index1, index2 = np.random.choice(num_features, size=2)\n","        new_features_order[index1], new_features_order[index2] = new_features_order[index2], new_features_order[index1]\n","        # apply the new order of features\n","        entry[:] = entry[new_features_order]\n","\n","  return (np.append(dataset, duplicate_data, axis=0), np.append(labels, duplicate_labels, axis=0))"],"metadata":{"id":"1L7Pv3dynEpN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Usage example**"],"metadata":{"id":"GyrVdx9dplLA"}},{"cell_type":"code","source":["percentage = .5\n","\n","polluted_dataset, polluted_labels = pollute_swapping(example_dataset, example_labels, percentage)"],"metadata":{"id":"fLoTO43_pnS7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example_dataset, example_labels"],"metadata":{"id":"0_qlO_dwprF9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704033289083,"user_tz":-60,"elapsed":7,"user":{"displayName":"Federico Toschi","userId":"03268067091644128077"}},"outputId":"33c8423f-9b5e-48de-8779-c7f8eb0badb3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[-0.94920925,  0.33568484, -1.41548407, -1.78576295, -0.48758674],\n","        [-3.35471844, -1.4895879 ,  0.47891255, -2.63812044,  0.25589926],\n","        [ 2.8429093 ,  0.83633122, -1.89775432,  1.68994257, -0.59224401],\n","        [ 0.80460305,  2.24711467, -3.28064942, -1.0454157 , -1.63415213],\n","        [-0.89816726,  1.39373349, -0.79674393,  0.17159317,  0.02322873]]),\n"," array([1, 0, 1, 0, 0]))"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["polluted_dataset, polluted_labels"],"metadata":{"id":"PHf_cqRDprZv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704033289083,"user_tz":-60,"elapsed":7,"user":{"displayName":"Federico Toschi","userId":"03268067091644128077"}},"outputId":"fd3d2722-0e53-4fbd-8f15-c9bf68e500e4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[-0.94920925,  0.33568484, -1.41548407, -1.78576295, -0.48758674],\n","        [-3.35471844, -1.4895879 ,  0.47891255, -2.63812044,  0.25589926],\n","        [ 2.8429093 ,  0.83633122, -1.89775432,  1.68994257, -0.59224401],\n","        [ 0.80460305,  2.24711467, -3.28064942, -1.0454157 , -1.63415213],\n","        [-0.89816726,  1.39373349, -0.79674393,  0.17159317,  0.02322873],\n","        [-3.28064942, -1.63415213,  0.80460305, -1.0454157 ,  2.24711467],\n","        [-0.89816726,  1.39373349,  0.17159317, -0.79674393,  0.02322873]]),\n"," array([1, 0, 1, 0, 0, 0, 0]))"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["#### Similarity Pollution Functions"],"metadata":{"id":"7I1jvhqekejM"}},{"cell_type":"markdown","source":["##### Helper Functions"],"metadata":{"id":"l5nKq349kjh3"}},{"cell_type":"markdown","source":["**Cosine Similarity**"],"metadata":{"id":"jRju08pplFg-"}},{"cell_type":"code","source":["def generate_cosine_similar_vector(original_vector, similarity):\n","    # Form the unit vector parallel to v:\n","    u = original_vector / la.norm(original_vector)\n","\n","    # This is a vector which will most likely not be parallel with v, so we can\n","    # use it to calculate a vector perpendicular to v\n","    r = np.random.normal(0, 1, len(original_vector))\n","\n","    # Form a vector perpendicular to v:\n","    # This vector will be used to build the right triangle\n","    u_perp = r - r.dot(u)*u\n","\n","    # Make it a unit vector:\n","    u_perp = u_perp / la.norm(u_perp)\n","\n","    # w is the linear combination of u and u_perp with coefficients costheta\n","    # u-component is equal to the cosine of the triangle (adjacent to the angle)\n","    # u_perp-component is equal to the sine\n","    # and sin(theta) = sqrt(1 - costheta**2), respectively:\n","    w = similarity*u + np.sqrt(1 - similarity**2)*u_perp\n","\n","    # Rescale vector to original size\n","    magnitude_original = np.linalg.norm(original_vector)\n","    magnitude_w = np.linalg.norm(w)\n","\n","    # Calculate the scaling factor\n","    scaling_factor = magnitude_original / magnitude_w\n","\n","    # Scale vector w\n","    scaled_vector_w = w * scaling_factor\n","\n","    return scaled_vector_w"],"metadata":{"id":"9RKvXAzBkiOO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Pearson Correlation**"],"metadata":{"id":"Y8nkaz5jlCQC"}},{"cell_type":"code","source":["def generate_pearson_correlated_vector(original_vector, correlation_target, tolerance=0.01, max_iterations=1000):\n","    \"\"\"\n","    Generate a vector that incrementally adjusts its Pearson correlation to the specified target with the original vector.\n","\n","    This is done by gradually adding noise until correlation drops to the desired target.\n","\n","    Parameters:\n","    original_vector (numpy.ndarray): The original vector to which the generated vector should be correlated.\n","    correlation_target (float): The target Pearson correlation coefficient between the original and the generated vector.\n","    tolerance (float): The acceptable deviation from the target correlation for the generated vector.\n","    max_iterations (int): The maximum number of iterations to try to achieve the target correlation.\n","\n","    Returns:\n","    numpy.ndarray: A vector that is incrementally adjusted to reach the specified Pearson correlation target.\n","    \"\"\"\n","    current_vector = np.array(original_vector, dtype=float)  # Ensure the vector is of float type for incremental adjustments\n","    original_norm = np.linalg.norm(original_vector)\n","\n","    for _ in range(max_iterations):\n","        # Generate small random noise\n","        noise = np.random.normal(0, 0.1, len(original_vector))\n","\n","        # Incrementally adjust the vector by adding noise\n","        current_vector += noise\n","\n","        # Normalize the adjusted vector to have the same magnitude as the original vector\n","        current_vector_norm = np.linalg.norm(current_vector)\n","        current_vector = (current_vector / current_vector_norm) * original_norm\n","\n","        # Calculate Pearson correlation with the original vector\n","        current_correlation, _ = pearsonr(original_vector, current_vector)\n","\n","        # Check if the current correlation is within the specified tolerance\n","        if abs(correlation_target - current_correlation) <= tolerance:\n","            return current_vector\n","\n","    return current_vector\n"],"metadata":{"id":"oDL8UqDHkvet"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**(Addition) Tanimoto Similarity**\n","\n","Tanimoto similarity for continuous variables is calculated as:\n","$T(a, b) = \\frac{A \\cdot B}{||A||^2 + ||B||^2 - A \\cdot B}$\n","\n","The goal is to find a vector $B$ such that the Tanimoto similarity with $A$ is the specified value $T$"],"metadata":{"id":"ZltCIvvzk3LT"}},{"cell_type":"code","source":["def tanimoto_similarity(vector_a, vector_b):\n","    dot_product = np.dot(vector_a, vector_b)\n","    # Refer to formula above\n","    return dot_product / (np.square(np.linalg.norm(vector_a)) + np.square(np.linalg.norm(vector_b)) - dot_product)"],"metadata":{"id":"yJrdaGEElIVD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_tanimoto_similar_vector(original_vector, similarity_target, tolerance=1e-5, max_iterations=1000):\n","    '''\n","    Generates a vector with specified Tanimoto similarity to an original vector.\n","\n","    This problem has no algebraic solution like with pearson's, so we need to use\n","    an iterative solution.\n","\n","    Take a random perturbation vector and add it to the original, then begin iterating:\n","    For each step, calculate an adjustment factor to scale the perturbation to be\n","    added to the new vector.\n","\n","    Repeat until the two vectors' tanimoto similarity goes below the threshold, or\n","    until the maximum numbers of iterations is reached.\n","\n","    Parameters:\n","      original_vector (np.ndarray): The original vector to which the generated vectors should be similar.\n","      similarity_target (float): The target Tanimoto similarity between the original and generated vectors.\n","      tolerance (float): The tolerance within which the generated vectors should match the target similarity.\n","      max_iterations (int): The maximum number of iterations to perform.\n","\n","    Returns:\n","      np.ndarray: A batch of vectors, each with a Tanimoto similarity to the original vector\n","                  close to the specified target.\n","    '''\n","    perturbation = np.random.normal(0, 0.1, len(original_vector))\n","    new_vector = original_vector + perturbation\n","    for _ in range(max_iterations):\n","        current_similarity = tanimoto_similarity(original_vector, new_vector)\n","        if abs(current_similarity - similarity_target) < tolerance:\n","            break\n","\n","        # Adjust new_vector\n","        adjustment_factor = (similarity_target - current_similarity) * 0.1  # A small step towards the target\n","        new_vector += adjustment_factor * perturbation  # Adjust based on the initial perturbation direction\n","\n","    return new_vector"],"metadata":{"id":"p9RGrJ7BkxdE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Pollution Function"],"metadata":{"id":"cpAKcE5Tkq9H"}},{"cell_type":"code","source":["similarity_functions = {'cosine': generate_cosine_similar_vector, 'pearson': generate_pearson_correlated_vector, 'tanimoto': generate_tanimoto_similar_vector}"],"metadata":{"id":"giG3l_kMlMQm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pollute_similarity(dataset, labels, pollution_percentage, similarity, similarity_percentage=0.9):\n","  if not (0 < percentage < 1):\n","    raise ValueError(\"Percentage must be between 0 and 1.\")\n","\n","  if similarity.lower() not in similarity_functions.keys():\n","    raise ValueError(f\"Similarity {similarity} not implemented.\")\n","\n","  num_entries, num_features = dataset.shape\n","  num_duplicates = int(num_entries * percentage)\n","\n","  if num_duplicates == 0:\n","    num_duplicates = 1\n","\n","  # Select random entries to duplicate and create a deep copy\n","  duplicate_indices = np.random.choice(num_entries, size=num_duplicates)\n","  duplicate_data = dataset[duplicate_indices].copy()\n","  duplicate_labels = labels[duplicate_indices].copy()\n","\n","  similarity_function = similarity_functions.get(similarity.lower())\n","\n","  for idx,entry in enumerate(duplicate_data):\n","    new_duplicate = similarity_function(entry, similarity_percentage)\n","    duplicate_data[idx] = new_duplicate\n","\n","  return (np.append(dataset, duplicate_data, axis=0), np.append(labels, duplicate_labels, axis=0))"],"metadata":{"id":"gdMyfn10lO7H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Usage Examples**"],"metadata":{"id":"peDBXf_qlSat"}},{"cell_type":"code","source":["percentage = .5\n","\n","polluted_dataset, polluted_labels = pollute_similarity(example_dataset, example_labels, percentage, 'cosine')"],"metadata":{"id":"Q6_YwiI2lVWM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example_dataset, example_labels"],"metadata":{"id":"mQfcdSHPlVUR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704033289449,"user_tz":-60,"elapsed":6,"user":{"displayName":"Federico Toschi","userId":"03268067091644128077"}},"outputId":"674b8dfe-7096-4333-c205-4236ee2f8a6e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[-0.94920925,  0.33568484, -1.41548407, -1.78576295, -0.48758674],\n","        [-3.35471844, -1.4895879 ,  0.47891255, -2.63812044,  0.25589926],\n","        [ 2.8429093 ,  0.83633122, -1.89775432,  1.68994257, -0.59224401],\n","        [ 0.80460305,  2.24711467, -3.28064942, -1.0454157 , -1.63415213],\n","        [-0.89816726,  1.39373349, -0.79674393,  0.17159317,  0.02322873]]),\n"," array([1, 0, 1, 0, 0]))"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["polluted_dataset, polluted_labels"],"metadata":{"id":"z4ymPJznlVSL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704033289449,"user_tz":-60,"elapsed":6,"user":{"displayName":"Federico Toschi","userId":"03268067091644128077"}},"outputId":"772a0d12-46ee-4bb3-b59c-37dd5822d979"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[-0.94920925,  0.33568484, -1.41548407, -1.78576295, -0.48758674],\n","        [-3.35471844, -1.4895879 ,  0.47891255, -2.63812044,  0.25589926],\n","        [ 2.8429093 ,  0.83633122, -1.89775432,  1.68994257, -0.59224401],\n","        [ 0.80460305,  2.24711467, -3.28064942, -1.0454157 , -1.63415213],\n","        [-0.89816726,  1.39373349, -0.79674393,  0.17159317,  0.02322873],\n","        [ 0.94731617,  1.32125332, -4.02009833,  0.47922708, -1.09039017],\n","        [-1.24481628, -0.71120455, -1.13013486, -1.73025637, -0.34292331]]),\n"," array([1, 0, 1, 0, 0, 0, 1]))"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["polluted_dataset, polluted_labels = pollute_similarity(example_dataset, example_labels, percentage, 'pearson')"],"metadata":{"id":"ruAFHE8olVQN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["polluted_dataset, polluted_labels"],"metadata":{"id":"Mcv6vAfOlVN7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704033289890,"user_tz":-60,"elapsed":12,"user":{"displayName":"Federico Toschi","userId":"03268067091644128077"}},"outputId":"bb22fe9b-1009-428b-939b-731944771b6d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[-0.94920925,  0.33568484, -1.41548407, -1.78576295, -0.48758674],\n","        [-3.35471844, -1.4895879 ,  0.47891255, -2.63812044,  0.25589926],\n","        [ 2.8429093 ,  0.83633122, -1.89775432,  1.68994257, -0.59224401],\n","        [ 0.80460305,  2.24711467, -3.28064942, -1.0454157 , -1.63415213],\n","        [-0.89816726,  1.39373349, -0.79674393,  0.17159317,  0.02322873],\n","        [-1.34150324,  0.47910634, -1.70882098, -1.1788211 , -0.32419998],\n","        [ 1.68234228,  1.10272655, -3.3536629 , -1.99290274, -0.97873419]]),\n"," array([1, 0, 1, 0, 0, 1, 0]))"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["polluted_dataset, polluted_labels = pollute_similarity(example_dataset, example_labels, percentage, 'tanimoto', similarity_percentage=0.9)"],"metadata":{"id":"AUeReSGOlVLk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["polluted_dataset, polluted_labels"],"metadata":{"id":"foiXNueulmMH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704033289891,"user_tz":-60,"elapsed":8,"user":{"displayName":"Federico Toschi","userId":"03268067091644128077"}},"outputId":"0014c475-8577-48c5-f7c3-25212f8e208f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[-0.94920925,  0.33568484, -1.41548407, -1.78576295, -0.48758674],\n","        [-3.35471844, -1.4895879 ,  0.47891255, -2.63812044,  0.25589926],\n","        [ 2.8429093 ,  0.83633122, -1.89775432,  1.68994257, -0.59224401],\n","        [ 0.80460305,  2.24711467, -3.28064942, -1.0454157 , -1.63415213],\n","        [-0.89816726,  1.39373349, -0.79674393,  0.17159317,  0.02322873],\n","        [ 2.81616623,  1.74564245, -1.52533741,  1.24381089, -1.12319871],\n","        [ 1.98430088,  0.85260431, -2.31459639,  1.18162291, -0.34215541]]),\n"," array([1, 0, 1, 0, 0, 1, 1]))"]},"metadata":{},"execution_count":43}]},{"cell_type":"markdown","source":["####Data pollution function\n","This function picks a percentage of entries from the dataset, applies the specified pollution function, and shuffles the result. It will be used in the following experiments."],"metadata":{"id":"ntJWUrrzPK3K"}},{"cell_type":"code","source":["def pollute_duplication(dataset, labels, percentage, pollution_function, **kwargs):\n","  '''\n","  Create duplicates from the dataset of size (percentage * dataset.shape[0]),\n","  pollutes them with the given pollution_function, and shuffle the result.\n","\n","  Parameters:\n","    dataset (numpy.ndarray): The dataset to be duplicated and polluted.\n","    labels (numpy.ndarray): The labels of the dataset.\n","    percentage (float): The fraction of the dataset size to duplicate.\n","    pollution_function (object): The pollution function to apply to the dataset.\n","    **kwargs: Additional keyword arguments for specific pollution functions.\n","\n","  Returns:\n","    numpy.ndarray: The polluted dataset with duplicates.\n","\n","  Raises:\n","    ValueError: If the dataset violates the pollution_function conditions.\n","  '''\n","\n","  polluted_dataset, polluted_labels = pollution_function(dataset, labels, percentage, **kwargs)\n","  polluted_labels = polluted_labels[:, np.newaxis]\n","  polluted_dataset_labels = np.concatenate((polluted_dataset, polluted_labels), axis=1)\n","\n","  # shuffle the dataset with the labels\n","  np.random.shuffle(polluted_dataset_labels)\n","\n","  num_features = polluted_dataset.shape[1]\n","\n","  return (polluted_dataset_labels[:, :num_features], polluted_dataset_labels[:, num_features:])"],"metadata":{"id":"aoAzOgtLPNmr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Usage example**"],"metadata":{"id":"4XKh6DyvQplQ"}},{"cell_type":"code","source":["percentage = .5\n","pollution_function = pollute_swapping\n","\n","polluted_dataset, polluted_labels = pollute_duplication(example_dataset, example_labels, percentage, pollution_function)"],"metadata":{"id":"JpQyuu_5QrgL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example_dataset, example_labels"],"metadata":{"id":"8C9zLBcQRV9r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704033289891,"user_tz":-60,"elapsed":7,"user":{"displayName":"Federico Toschi","userId":"03268067091644128077"}},"outputId":"9a589caa-d701-4c62-a622-b8ffd460bc43"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[-0.94920925,  0.33568484, -1.41548407, -1.78576295, -0.48758674],\n","        [-3.35471844, -1.4895879 ,  0.47891255, -2.63812044,  0.25589926],\n","        [ 2.8429093 ,  0.83633122, -1.89775432,  1.68994257, -0.59224401],\n","        [ 0.80460305,  2.24711467, -3.28064942, -1.0454157 , -1.63415213],\n","        [-0.89816726,  1.39373349, -0.79674393,  0.17159317,  0.02322873]]),\n"," array([1, 0, 1, 0, 0]))"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["polluted_dataset, polluted_labels"],"metadata":{"id":"4tYWIbpWRZCT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704033289891,"user_tz":-60,"elapsed":7,"user":{"displayName":"Federico Toschi","userId":"03268067091644128077"}},"outputId":"4314a9ae-fb96-414e-c3de-455c18880e79"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[-3.35471844, -2.63812044, -1.4895879 ,  0.47891255,  0.25589926],\n","        [ 2.24711467, -1.63415213,  0.80460305, -3.28064942, -1.0454157 ],\n","        [-0.94920925,  0.33568484, -1.41548407, -1.78576295, -0.48758674],\n","        [-0.89816726,  1.39373349, -0.79674393,  0.17159317,  0.02322873],\n","        [ 2.8429093 ,  0.83633122, -1.89775432,  1.68994257, -0.59224401],\n","        [-3.35471844, -1.4895879 ,  0.47891255, -2.63812044,  0.25589926],\n","        [ 0.80460305,  2.24711467, -3.28064942, -1.0454157 , -1.63415213]]),\n"," array([[0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.]]))"]},"metadata":{},"execution_count":47}]},{"cell_type":"markdown","source":["### Experiments"],"metadata":{"id":"oKgWhM0ekKVH"}},{"cell_type":"markdown","source":["####Experiments dataset initialization"],"metadata":{"id":"B8NDUh3C4FSe"}},{"cell_type":"code","source":["X_duplication_experiments = []\n","y_duplication_experiments = []"],"metadata":{"id":"88aVKf2e4Mzj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Experiment \\#1\n","The dataset will be **slightly** polluted with the **rounding-off** pollution function."],"metadata":{"id":"tIELDPwPxVE6"}},{"cell_type":"code","source":["dataset = X_dup\n","labels = y_dup\n","percentage = .05\n","pollution_function = pollute_round_off\n","\n","X_polluted, y_polluted = pollute_duplication(dataset, labels, percentage, pollution_function)\n","X_duplication_experiments.append(X_polluted)\n","y_duplication_experiments.append(y_polluted)\n","X_polluted, y_polluted"],"metadata":{"id":"Y6bi57bMxX7M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704033289891,"user_tz":-60,"elapsed":6,"user":{"displayName":"Federico Toschi","userId":"03268067091644128077"}},"outputId":"09f158cd-369a-4acb-a367-f5954187eaa1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[-1.26362418,  1.72283532,  2.29646349,  0.86979494, -0.19620279],\n","        [-0.55128057,  1.82366901,  1.04789728, -1.43508087, -1.8044368 ],\n","        [-1.44794874,  0.94356927,  0.31017483, -1.58841984, -0.82191895],\n","        ...,\n","        [ 2.07126709,  0.84328634, -1.44131217, -0.9828732 ,  0.76882991],\n","        [-0.64406719,  1.97148781,  2.35837219,  1.76982818,  0.28841226],\n","        [-1.72869558, -0.66054133,  0.83074544, -0.29965793,  0.76912222]]),\n"," array([[1.],\n","        [1.],\n","        [1.],\n","        ...,\n","        [0.],\n","        [1.],\n","        [1.]]))"]},"metadata":{},"execution_count":49}]},{"cell_type":"markdown","source":["####Experiment \\#2\n","The dataset will be **heavily** polluted with the **rounding-off** pollution function."],"metadata":{"id":"A3TxFnfXxxjC"}},{"cell_type":"code","source":["dataset = X_dup\n","labels = y_dup\n","percentage = .5\n","pollution_function = pollute_round_off\n","\n","X_polluted, y_polluted = pollute_duplication(dataset, labels, percentage, pollution_function)\n","X_duplication_experiments.append(X_polluted)\n","y_duplication_experiments.append(y_polluted)\n","X_polluted, y_polluted"],"metadata":{"id":"um1eTSnTx-Ix","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704033289891,"user_tz":-60,"elapsed":6,"user":{"displayName":"Federico Toschi","userId":"03268067091644128077"}},"outputId":"fd1300ba-ac94-4a47-a88a-0b031b69fe93"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[-1.28226799,  1.06877017, -0.38059387,  1.81219199,  3.03842247],\n","        [-0.40673444, -1.64746513,  0.62564852, -1.50362343,  1.06173768],\n","        [-1.63800694, -0.1154045 ,  1.68829322, -1.1079689 , -0.01318753],\n","        ...,\n","        [ 0.15467695, -0.19948894, -2.13626932, -2.03410944, -3.91796758],\n","        [ 2.02971912,  1.28201321,  2.0545469 ,  0.95644835,  1.6759338 ],\n","        [ 0.08835475, -1.33169351, -0.90085005, -0.82116032,  0.73641052]]),\n"," array([[1.],\n","        [0.],\n","        [0.],\n","        ...,\n","        [0.],\n","        [1.],\n","        [0.]]))"]},"metadata":{},"execution_count":50}]},{"cell_type":"markdown","source":["####Experiment \\#3\n","The dataset will be **slightly** polluted with the **Gaussian noise** pollution function."],"metadata":{"id":"wObYbu0OyQef"}},{"cell_type":"code","source":["dataset = X_dup\n","labels = y_dup\n","percentage = .05\n","pollution_function = pollute_gaussian_noise\n","\n","X_polluted, y_polluted = pollute_duplication(dataset, labels, percentage, pollution_function)\n","X_duplication_experiments.append(X_polluted)\n","y_duplication_experiments.append(y_polluted)\n","X_polluted, y_polluted"],"metadata":{"id":"Mku0tEzqyUH1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704033290218,"user_tz":-60,"elapsed":333,"user":{"displayName":"Federico Toschi","userId":"03268067091644128077"}},"outputId":"d838a1d7-d254-43b3-e952-06f3e36152c6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[ 1.16327706, -1.05028594,  0.65234844,  0.93632615,  2.97651156],\n","        [ 0.91014726, -1.39476408, -0.00675588, -1.66315581,  1.4806439 ],\n","        [ 3.43483512,  1.16804449, -1.4676696 , -1.20489941,  1.33947855],\n","        ...,\n","        [-1.67281789, -0.07856119,  2.16641976, -0.38347766, -0.29164429],\n","        [-1.73033603,  0.86148643,  1.96462253, -0.61253313, -1.51461633],\n","        [-2.71417002,  1.88932855, -1.71647742, -2.11834123,  1.12844336]]),\n"," array([[0.],\n","        [0.],\n","        [0.],\n","        ...,\n","        [1.],\n","        [1.],\n","        [0.]]))"]},"metadata":{},"execution_count":51}]},{"cell_type":"markdown","source":["####Experiment \\#4\n","The dataset will be **heavily** polluted with the **Gaussian noise** pollution function."],"metadata":{"id":"tYyg1BsVyor-"}},{"cell_type":"code","source":["dataset = X_dup\n","labels = y_dup\n","percentage = .5\n","pollution_function = pollute_gaussian_noise\n","\n","X_polluted, y_polluted = pollute_duplication(dataset, labels, percentage, pollution_function)\n","X_duplication_experiments.append(X_polluted)\n","y_duplication_experiments.append(y_polluted)\n","X_polluted, y_polluted"],"metadata":{"id":"JPRn7qVOytt2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704033290218,"user_tz":-60,"elapsed":6,"user":{"displayName":"Federico Toschi","userId":"03268067091644128077"}},"outputId":"ae526456-36d8-4ed4-dfac-6b0b501719ec"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[-1.70804736,  1.25213318,  2.08602872, -1.24431503, -2.41561583],\n","        [ 1.33532555,  3.7805688 ,  0.16231271, -2.04894238, -3.07922867],\n","        [-1.90050182,  1.22551263,  1.42378072, -1.59741314, -1.88375507],\n","        ...,\n","        [-1.38362686,  2.00271224,  1.63428037, -0.6510599 , -2.98146146],\n","        [-0.78407297,  1.95310397,  2.17423237,  1.89704651,  0.44785354],\n","        [ 0.88226638, -1.11451841,  0.52829617,  0.34566745,  2.63255647]]),\n"," array([[1.],\n","        [1.],\n","        [1.],\n","        ...,\n","        [1.],\n","        [1.],\n","        [0.]]))"]},"metadata":{},"execution_count":52}]},{"cell_type":"markdown","source":["####Experiment \\#5\n","The dataset will be **slightly** polluted with the **scaling** pollution function."],"metadata":{"id":"cHGJzNsQy8IA"}},{"cell_type":"code","source":["dataset = X_dup\n","labels = y_dup\n","percentage = .05\n","pollution_function = pollute_scaling\n","\n","X_polluted, y_polluted = pollute_duplication(dataset, labels, percentage, pollution_function)\n","X_duplication_experiments.append(X_polluted)\n","y_duplication_experiments.append(y_polluted)\n","X_polluted, y_polluted"],"metadata":{"id":"-wx_MWuoy-kQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704033290218,"user_tz":-60,"elapsed":6,"user":{"displayName":"Federico Toschi","userId":"03268067091644128077"}},"outputId":"b9d0e8a8-66e7-4154-bc90-68f25a360183"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[-1.57418806,  1.05993288,  0.53385234,  0.13397501,  1.13179834],\n","        [-1.99034914, -0.08653247,  1.53060829,  1.09700743, -0.64246656],\n","        [ 1.6366213 , -0.23566977, -1.17779344, -1.1118078 ,  0.98667669],\n","        ...,\n","        [ 1.30781299, -1.37981411, -1.56062445, -0.80202517,  1.13028903],\n","        [ 0.22835649,  2.33104715,  2.25816762,  0.44672085,  1.14652017],\n","        [-0.99554243,  0.5664292 ,  0.96044104, -0.31096013, -0.51153448]]),\n"," array([[1.],\n","        [1.],\n","        [0.],\n","        ...,\n","        [0.],\n","        [1.],\n","        [1.]]))"]},"metadata":{},"execution_count":53}]},{"cell_type":"markdown","source":["####Experiment \\#6\n","The dataset will be **heavily** polluted with the **scaling** pollution function."],"metadata":{"id":"E24dr21bzK1e"}},{"cell_type":"code","source":["dataset = X_dup\n","labels = y_dup\n","percentage = .5\n","pollution_function = pollute_scaling\n","\n","X_polluted, y_polluted = pollute_duplication(dataset, labels, percentage, pollution_function)\n","X_duplication_experiments.append(X_polluted)\n","y_duplication_experiments.append(y_polluted)\n","X_polluted, y_polluted"],"metadata":{"id":"7oZaPg6HzOvd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704033290218,"user_tz":-60,"elapsed":6,"user":{"displayName":"Federico Toschi","userId":"03268067091644128077"}},"outputId":"254a2fb1-f96f-47f5-99d1-be81ed24dd74"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[ 7.21617465e-02,  8.18653615e-03, -6.12299458e-02,\n","         -1.27659443e-01, -3.48661964e-01],\n","        [-1.47016681e+00,  6.04944359e-01,  1.85924642e+00,\n","          1.49491367e-01, -1.11689240e+00],\n","        [-1.21027842e+00,  2.37150593e+00,  1.41051880e+00,\n","          2.57274582e+00,  1.50165244e+00],\n","        ...,\n","        [-3.07265909e+01, -2.21699480e+00,  3.00032276e+01,\n","          7.34709247e+00, -1.40978286e+01],\n","        [-2.18897900e-05,  2.26398205e-05,  4.30963584e-05,\n","          1.64826781e-04,  1.37803680e-04],\n","        [-1.59857854e-01,  1.45972006e+00,  2.21790880e+00,\n","         -5.28451756e-01,  2.13062255e-01]]),\n"," array([[0.],\n","        [1.],\n","        [1.],\n","        ...,\n","        [1.],\n","        [1.],\n","        [1.]]))"]},"metadata":{},"execution_count":54}]},{"cell_type":"markdown","source":["####Experiment \\#7\n","The dataset will be **slightly** polluted with the **swapping** pollution function."],"metadata":{"id":"QbiGW_WPzaR_"}},{"cell_type":"code","source":["dataset = X_dup\n","labels = y_dup\n","percentage = .05\n","pollution_function = pollute_swapping\n","\n","X_polluted, y_polluted = pollute_duplication(dataset, labels, percentage, pollution_function)\n","X_duplication_experiments.append(X_polluted)\n","y_duplication_experiments.append(y_polluted)\n","X_polluted, y_polluted"],"metadata":{"id":"Mh7r5BjBzeCE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704033290218,"user_tz":-60,"elapsed":5,"user":{"displayName":"Federico Toschi","userId":"03268067091644128077"}},"outputId":"3788215d-39ef-41d5-d952-3637f5fb223c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[ 1.87073804, -0.52367569, -1.2353938 , -1.04128874,  1.21386213],\n","        [ 1.14422585, -0.40023995,  1.04788291,  2.27577602,  3.24797063],\n","        [-0.32451768,  1.97100488,  2.55645075,  3.19597299,  0.4822227 ],\n","        ...,\n","        [-0.82545885,  0.1631802 , -1.71975096, -0.06591352, -2.04844101],\n","        [ 0.75143652,  0.44646148, -0.08478494, -3.23481762,  1.58516593],\n","        [-1.90050182,  1.22551263,  1.42378072, -1.59741314, -1.88375507]]),\n"," array([[0.],\n","        [0.],\n","        [1.],\n","        ...,\n","        [0.],\n","        [1.],\n","        [1.]]))"]},"metadata":{},"execution_count":55}]},{"cell_type":"markdown","source":["####Experiment \\#8\n","The dataset will be **heavily** polluted with the **swapping** pollution function."],"metadata":{"id":"jlmbtQBlzm3w"}},{"cell_type":"code","source":["dataset = X_dup\n","labels = y_dup\n","percentage = .5\n","pollution_function = pollute_swapping\n","\n","X_polluted, y_polluted = pollute_duplication(dataset, labels, percentage, pollution_function)\n","X_duplication_experiments.append(X_polluted)\n","y_duplication_experiments.append(y_polluted)\n","X_polluted, y_polluted"],"metadata":{"id":"3e5wVJ0KzqEq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704033290218,"user_tz":-60,"elapsed":5,"user":{"displayName":"Federico Toschi","userId":"03268067091644128077"}},"outputId":"a51fc2e0-feab-48e1-d5dd-e7aeac3b5853"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[-2.22753413, -2.95910976, -1.40963308, -1.39155788, -1.80300123],\n","        [-1.60467918, -0.4877585 , -0.19529525,  0.16920395,  1.05103003],\n","        [-1.2457026 , -0.12868055, -2.63781432, -1.085902  , -2.7673109 ],\n","        ...,\n","        [-1.57637239,  0.36012162,  0.20470767,  0.09888187,  0.23829669],\n","        [ 1.9494065 ,  0.29528   , -1.10607936,  0.10053975,  1.42604526],\n","        [-0.37448419,  2.5908915 , -2.91648514,  1.48902087, -0.89080027]]),\n"," array([[0.],\n","        [1.],\n","        [0.],\n","        ...,\n","        [1.],\n","        [0.],\n","        [1.]]))"]},"metadata":{},"execution_count":56}]},{"cell_type":"markdown","source":["#### Experiment \\#9\n","\n","The dataset will be **slightly** polluted with the **similarity** pollution function, using the *Tanimoto* similarity measure, with **75%** similarity"],"metadata":{"id":"ltMhTTKcjota"}},{"cell_type":"code","source":["dataset = X_dup\n","labels = y_dup\n","percentage = .05\n","pollution_function = pollute_similarity\n","\n","X_polluted, y_polluted = pollute_duplication(dataset, labels, percentage, pollution_function, similarity='tanimoto', similarity_percentage=0.75)\n","X_duplication_experiments.append(X_polluted)\n","y_duplication_experiments.append(y_polluted)\n","X_polluted, y_polluted"],"metadata":{"id":"Ty2yOZ-Ojubx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704033291201,"user_tz":-60,"elapsed":987,"user":{"displayName":"Federico Toschi","userId":"03268067091644128077"}},"outputId":"684c07b2-afc4-4f19-c19a-771e24301afb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[-3.36369582, -0.56942765,  0.77089656,  0.65329003, -0.59504281],\n","        [ 0.8479711 ,  1.29572189, -1.5157126 , -0.98433789, -2.83446981],\n","        [ 1.28623488, -0.60008139, -0.77648608, -1.92129938,  0.54265603],\n","        ...,\n","        [ 0.75143652,  0.44646148, -0.08478494, -3.23481762,  1.58516593],\n","        [ 2.5776916 ,  0.90093164, -0.94424682, -0.07889503,  1.62874676],\n","        [-0.56724982, -0.65374864, -3.43827305, -1.98789721, -4.56815193]]),\n"," array([[1.],\n","        [0.],\n","        [0.],\n","        ...,\n","        [1.],\n","        [0.],\n","        [0.]]))"]},"metadata":{},"execution_count":57}]},{"cell_type":"markdown","source":["#### Experiment \\#10\n","\n","The dataset will be **heavily** polluted with the **similarity** pollution function, using the *Tanimoto* correlation measure, with **75%** similarity"],"metadata":{"id":"qWCEuukijsXB"}},{"cell_type":"code","source":["dataset = X_dup\n","labels = y_dup\n","percentage = .5\n","pollution_function = pollute_similarity\n","\n","X_polluted, y_polluted = pollute_duplication(dataset, labels, percentage, pollution_function, similarity='tanimoto', similarity_percentage=0.75)\n","X_duplication_experiments.append(X_polluted)\n","y_duplication_experiments.append(y_polluted)\n","X_polluted, y_polluted"],"metadata":{"id":"3-hAXXJGjvF8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704033308013,"user_tz":-60,"elapsed":16816,"user":{"displayName":"Federico Toschi","userId":"03268067091644128077"}},"outputId":"5b6cdc30-8798-42ad-a608-3b9f01e154f0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[-2.62362924,  2.54845081, -0.7149473 , -2.65296162, -1.44657024],\n","        [-2.22753413, -2.95910976, -1.40963308, -1.39155788, -1.80300123],\n","        [-2.09963073,  1.03158183, -0.10404743, -2.25832871,  1.46534062],\n","        ...,\n","        [ 1.46189474, -0.87546186,  0.09550884, -1.50931106,  1.57459298],\n","        [ 0.17042123, -2.04626726, -2.28031576, -1.25690438, -4.65496396],\n","        [-1.41513393,  1.44701509,  1.57860873, -0.94710872, -2.07719494]]),\n"," array([[1.],\n","        [0.],\n","        [0.],\n","        ...,\n","        [0.],\n","        [0.],\n","        [1.]]))"]},"metadata":{},"execution_count":58}]}]}